{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Z1voPA0zLJ0I"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2024 Google LLC. Licensed under the Apache License, Version 2.0 (the \"License\");"
      ],
      "metadata": {
        "id": "TVZlb_WyecAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License"
      ],
      "metadata": {
        "id": "FHdkP3HoedlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "YHgjY5eb7TBE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 1: Get the Embeddings: The following cells in this notebook will access the PD-Foundations embeddings directly from a BigQuery table.\n",
        "\n",
        "⚠️ Important: To run these cells successfully, you must first obtain access to the embeddings dataset via a BigQuery Listing. Please use [this form](https://forms.gle/ysdp5uUoPrMrhjZQA) to apply for research access or to join the Early Access Program waitlist.\n",
        "\n",
        "Once your access is approved, ensure you are authenticated in this Colab with a Google account that has permission to query the BigQuery table...."
      ],
      "metadata": {
        "id": "VzNXfZyVS5wY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Ky58xlzJfPJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = '<project_id>' # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "jWU-nUAYkiSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pandas_gbq\n",
        "\n",
        "\n",
        "# @markdown Specify the BigQuery table ID for the county embeddings.\n",
        "# Example: 'your-gcp-project.your_dataset.your_table'\n",
        "bigquery_table_id = '<project_id>.pdfm_embeddings.us_embeddings_v0' # @param {type:\"string\"}\n",
        "\n",
        "query = f\"SELECT * FROM `{bigquery_table_id}` WHERE region_type='county'\"\n",
        "\n",
        "# Load data from BigQuery into a pandas DataFrame.\n",
        "county_embeddings = pandas_gbq.read_gbq(query, project_id=PROJECT_ID, dialect='standard').set_index('place_name')"
      ],
      "metadata": {
        "id": "POKuIJCRkuU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Specify the BigQuery table ID for the zcta embeddings.\n",
        "bigquery_table_id = '<project_id>.pdfm_embeddings.us_embeddings_v0' # @param {type:\"string\"}\n",
        "query = f\"SELECT * FROM `{bigquery_table_id}` WHERE region_type='postal_code'\"\n",
        "zip_embeddings = pandas_gbq.read_gbq(query, project_id=PROJECT_ID, dialect='standard').set_index('place_name')\n"
      ],
      "metadata": {
        "id": "pZL03aEolN_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = pd.concat([county_embeddings, zip_embeddings])"
      ],
      "metadata": {
        "id": "v2G7WlNsmpGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The colab uses geojson files which are available in the github repo under data folder. Download the geojson file into a local folder or a folder under Google drive. Here we assume that you have downloaded the file in Google Drive folder called pdfm_embeddings/v0/us."
      ],
      "metadata": {
        "id": "hsp3yGaEm8rF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lC7wR5HD5RVR"
      },
      "outputs": [],
      "source": [
        "#@markdown Specify the path to the geo folder.\n",
        "BASE_PATH = '/content/drive/MyDrive/pdfm_embeddings/v0/us/' #@param {type:\"string\"}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_features = [f'feature{x}' for x in range(330)]\n",
        "embeddings.head(2)"
      ],
      "metadata": {
        "id": "V_4Lt8Yl5vDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings.index"
      ],
      "metadata": {
        "id": "MsNHu8Bzn3k2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 2: Download and load a few variables from data commons."
      ],
      "metadata": {
        "id": "V15mXezNgezz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datacommons_pandas --upgrade --quiet\n",
        "import datacommons_pandas as dc"
      ],
      "metadata": {
        "id": "MoHVllmrgtHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This may take a few minutes to run.\n",
        "labels = ['Count_Person',\n",
        "          'Count_Person_EducationalAttainmentBachelorsDegreeOrHigher',\n",
        "          'Median_Age_Person',\n",
        "          'Median_Income_Household',\n",
        "          'Percent_Person_WithAsthma',\n",
        "          'Percent_Person_WithHighBloodPressure'\n",
        "          ]\n",
        "df_labels = dc.build_multivariate_dataframe(embeddings.index, labels)\n",
        "print(df_labels.shape)\n",
        "df_labels.head(2)"
      ],
      "metadata": {
        "id": "h912C_ulguNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = embeddings.join(df_labels)\n",
        "df.head(1)"
      ],
      "metadata": {
        "id": "Xv9K0UJSh7at"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['state'] = df['location_metadata'].apply(lambda x: x.get('administrative_area_level1'))\n",
        "df['county'] = df['location_metadata'].apply(lambda x: x.get('administrative_area_level2'))\n"
      ],
      "metadata": {
        "id": "YfsMg9mWpNsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Percent_Person_WithHigherEdu'] = (df.Count_Person_EducationalAttainmentBachelorsDegreeOrHigher / df.Count_Person) * 100\n",
        "df['county_id'] = df['county'] + df['state']"
      ],
      "metadata": {
        "id": "IInbt3uwiCyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FBvuik32iGaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Visualizations"
      ],
      "metadata": {
        "id": "OOnmR8lhg8VF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the county and zcta (Zipcode census tabulation area) level geojson file.\n",
        "\n",
        "The county and zcta level geojson file are available in the same folder as the embeddings. Download the geojson file into a local folder or a folder under Google drive. Here we assume that you have downloaded the file in Google Drive folder called pdfm_embeddings/v0/us"
      ],
      "metadata": {
        "id": "kOI8jZyCG33q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "county_geo = gpd.read_file(BASE_PATH + 'county.geojson')\n",
        "zip_geo = gpd.read_file(BASE_PATH + 'zcta.geojson')"
      ],
      "metadata": {
        "id": "WvfGKA4kGTCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "geo = pd.concat([county_geo, zip_geo]).rename(columns={'place': 'place_name'}).set_index('place_name')\n",
        "embeddings = gpd.GeoDataFrame(embeddings, geometry=geo.geometry)\n",
        "embeddings.shape"
      ],
      "metadata": {
        "id": "aa4yagcBHo-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings.head(10)"
      ],
      "metadata": {
        "id": "7RMAtywvfOMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Map out an embedding dimension spatially"
      ],
      "metadata": {
        "id": "Z1voPA0zLJ0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_locale(df, index, states=None, counties=None):\n",
        "  df = df[df.index.isin(index)]\n",
        "  if not states and not counties:\n",
        "    return df\n",
        "  filter = df['location_metadata'].apply(lambda x: x.get('administrative_area_level1')).isin(states)\n",
        "  if counties:\n",
        "    filter &= df['location_metadata'].apply(lambda x: x.get('administrative_area_level2')).isin(counties)\n",
        "  return df[filter]"
      ],
      "metadata": {
        "id": "BHctv_WXiNcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Map out an embedding dimension feature0 spatially across all counties in US\n",
        "feature_name_to_plot = embedding_features[0]\n",
        "\n",
        "# Extract the specific feature from the 'features' array column into a new column for plotting\n",
        "feature_index = int(feature_name_to_plot.replace('feature', ''))\n",
        "embeddings[feature_name_to_plot] = embeddings['features'].apply(lambda x: x[feature_index])\n",
        "\n",
        "ax = get_locale(embeddings, county_embeddings.index).plot(feature_name_to_plot)\n",
        "_ = ax.set_title(feature_name_to_plot + ' in counties')"
      ],
      "metadata": {
        "id": "yaZ2_936KTQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "county_embeddings.columns"
      ],
      "metadata": {
        "id": "C8iq_XJorh41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Map out an embedding dimension feature0 spatially across all counties and zipcodes in NY state\n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
        "state = 'New York'\n",
        "get_locale(embeddings, county_embeddings.index, states=[state]).plot(feature_name_to_plot, ax=ax[0])\n",
        "get_locale(embeddings, zip_embeddings.index, states=[state]).plot(feature_name_to_plot, ax=ax[1])\n",
        "fig.suptitle(f'{feature_name_to_plot} in {state}')\n",
        "ax[0].set(title='counties')\n",
        "ax[1].set(title='zip codes')\n",
        "plt.setp(ax, xticks=[], yticks=[])\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "7WQHnSLyKxMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Map out a prediction variable spatially across all counties and zipcodes in NY state\n",
        "df = gpd.GeoDataFrame(df, geometry=embeddings.geometry)\n",
        "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
        "feature = 'Percent_Person_WithHigherEdu'\n",
        "state = 'New York'\n",
        "get_locale(df, county_embeddings.index, states=[state]).plot(feature,\n",
        "    legend=True, ax=ax[0])\n",
        "get_locale(df, zip_embeddings.index, states=[state]).plot(feature,\n",
        "    legend=True, ax=ax[1])\n",
        "fig.suptitle(f'{feature} in {state}')\n",
        "ax[0].set(title='counties')\n",
        "ax[1].set(title='zip codes')\n",
        "plt.setp(ax, xticks=[], yticks=[])\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "0o7eaWUvfXDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applying the embeddings in a prediction task"
      ],
      "metadata": {
        "id": "6jGB-wETlYFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Common imports and eval methods\n",
        "import numpy as np\n",
        "import math\n",
        "import sklearn.metrics as skmetrics\n",
        "from sklearn import linear_model, preprocessing\n",
        "import lightgbm as lgbm\n",
        "\n",
        "def evaluate(df: pd.DataFrame) -> dict:\n",
        "    \"\"\"Evaluates the model performance on the given dataframe.\n",
        "\n",
        "    Args:\n",
        "        df: A pandas DataFrame with columns 'y' and 'y_pred'.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary of performance metrics.\n",
        "    \"\"\"\n",
        "    # Ensure necessary columns exist and drop rows with NaN or zero in 'y'\n",
        "    if not {'y', 'y_pred'}.issubset(df.columns):\n",
        "        raise ValueError(\"DataFrame must contain 'y' and 'y_pred' columns\")\n",
        "\n",
        "    df = df.dropna(subset=['y', 'y_pred'])\n",
        "    df = df[df['y'] != 0]\n",
        "\n",
        "    r2 = skmetrics.r2_score(df['y'], df['y_pred'])\n",
        "    correlation = df['y'].corr(df['y_pred'])\n",
        "    rmse = math.sqrt(skmetrics.mean_squared_error(df['y'], df['y_pred']))\n",
        "    mae = skmetrics.mean_absolute_error(df['y'], df['y_pred'])\n",
        "    mape = skmetrics.mean_absolute_percentage_error(df['y'], df['y_pred'])\n",
        "\n",
        "    return {'r2': r2, 'rmse': rmse, 'mae': mae, 'mape': mape, 'correlation': correlation}\n",
        "\n",
        "\n",
        "def subset_eval(label: str, county_name: str, state: str, gpred: gpd.GeoDataFrame,\n",
        "                visualize: bool = True, cmap: str = 'Greys') -> dict:\n",
        "    \"\"\"Runs intra-county or intra-state evaluation and visualizes the results.\n",
        "\n",
        "    Args:\n",
        "        label: The label for the title of the visualization.\n",
        "        county_name: The specific county name to filter.\n",
        "        state: The specific state name to filter.\n",
        "        gpred: GeoDataFrame containing 'y', 'y_pred', 'state', and 'county' columns.\n",
        "        visualize: Whether to display visualizations.\n",
        "        cmap: Colormap for visualizations.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary of performance metrics.\n",
        "    \"\"\"\n",
        "    # Apply filters based on state and county name\n",
        "    subset = gpred.copy()\n",
        "    if state:\n",
        "        subset = subset[subset['state'] == state]\n",
        "    if county_name:\n",
        "        subset = subset[subset['county'] == county_name]\n",
        "\n",
        "    # Drop rows where 'y' is NaN\n",
        "    subset = subset.dropna(subset=['y', 'y_pred'])\n",
        "    eval_metrics = evaluate(subset)\n",
        "\n",
        "    if visualize:\n",
        "        _, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
        "\n",
        "        # Scatter plot of predicted vs actual\n",
        "        subset.plot.scatter('y', 'y_pred', alpha=0.8, ax=ax[2], color='darkgray')\n",
        "        x0, x1 = subset[['y', 'y_pred']].min().min(), subset[['y', 'y_pred']].max().max()\n",
        "        ax[2].plot([x0, x1], [x0, x1], ls='--', color='black')\n",
        "        ax[2].set_title(f'r={eval_metrics[\"correlation\"]:.2f}, mae={eval_metrics[\"mae\"]:.2f}')\n",
        "\n",
        "        # Maps of actual and predicted values\n",
        "        subset.plot('y', legend=True, ax=ax[0], vmin=x0, vmax=x1, cmap=cmap,\n",
        "                    legend_kwds={'fraction': 0.02, 'pad': 0.05})\n",
        "        ax[0].set_title('Actual')\n",
        "        subset.plot('y_pred', legend=False, ax=ax[1], vmin=x0, vmax=x1, cmap=cmap)\n",
        "        ax[1].set_title('Predicted')\n",
        "\n",
        "        plt.setp(ax[:2], xticks=[], yticks=[])\n",
        "        plt.suptitle(f'{label} - {county_name}, {state}')\n",
        "        plt.tight_layout()\n",
        "\n",
        "    return eval_metrics\n",
        "\n",
        "\n",
        "def make_predictions_df(predictions: np.ndarray, test_df: gpd.GeoDataFrame, label: str) -> gpd.GeoDataFrame:\n",
        "    \"\"\"Creates a GeoDataFrame with predictions, true labels, and geographic info.\n",
        "\n",
        "    Args:\n",
        "        predictions: A sequence of predictions.\n",
        "        test_df: The original test GeoDataFrame that the predictions are based on.\n",
        "        label: The column name for the true label in `test_df`.\n",
        "\n",
        "    Returns:\n",
        "        A GeoDataFrame for evaluation and visualizations.\n",
        "    \"\"\"\n",
        "    if label not in test_df.columns:\n",
        "        raise ValueError(f\"The specified label '{label}' does not exist in test_df columns.\")\n",
        "\n",
        "    df_predictions = pd.DataFrame({'y': test_df[label], 'y_pred': predictions}, index=test_df.index)\n",
        "    return test_df[['geometry', 'Count_Person', 'state', 'county']].join(df_predictions)\n"
      ],
      "metadata": {
        "id": "jhVnsHRUnmAc",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Superresolution - Train the model on counties and make predictions for zip code.\n"
      ],
      "metadata": {
        "id": "qQ55ASjJlgNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label = 'Percent_Person_WithHigherEdu'\n",
        "df['population'] = df['location_metadata'].apply(lambda x: x.get('population'))\n",
        "data = df[df[label].notna() & (df['population'] > 500)]\n",
        "\n",
        "# Extract individual features from the 'features' array into separate columns in 'data'\n",
        "# This is crucial for the model to access individual features.\n",
        "features_df = pd.DataFrame(data['features'].tolist(), index=data.index, columns=embedding_features)\n",
        "data = data.join(features_df)\n",
        "\n",
        "train = data[data.index.isin(county_embeddings.index)]\n",
        "test = data[data.index.isin(zip_embeddings.index)]\n",
        "\n",
        "model = linear_model.Ridge()\n",
        "model.fit(train[embedding_features], train[label])\n",
        "predictions = model.predict(test[embedding_features])\n",
        "gdf_predictions = make_predictions_df(predictions, test, label)\n",
        "evaluate(gdf_predictions)"
      ],
      "metadata": {
        "id": "Vey-QWY7lOvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualize some test set predictions\n",
        "_ = subset_eval(label, 'Harris County', 'Texas', gdf_predictions, cmap='Blues')\n",
        "_ = subset_eval(label, 'Greenville County', 'South Carolina', gdf_predictions, cmap='Blues')"
      ],
      "metadata": {
        "id": "MG8RTdsjpAuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Evaluate over a state by setting the county to an empty string.\n",
        "_ = subset_eval(label, '', 'New York', gdf_predictions, cmap='Blues')"
      ],
      "metadata": {
        "id": "02C2CbCdp1l0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imputation - zip -> zip\n",
        "Train on zipcodes in a subset of counties."
      ],
      "metadata": {
        "id": "uohbUjMZyD63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title train on zip codes in 20% of the counties, test on the remaining 80%.\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import sklearn.preprocessing\n",
        "\n",
        "def get_train_test_split(training_fraction=0.8):\n",
        "  \"\"\"Splits the data into training and testing sets based on county IDs.\"\"\"\n",
        "  data = df[df.index.isin(zip_embeddings.index)].copy()\n",
        "\n",
        "  # Split the zip codes by county into train/test sets.\n",
        "  train_counties = data.drop_duplicates('county_id').sample(\n",
        "      frac=training_fraction).county_id\n",
        "  train = data[data.county_id.isin(train_counties)].copy()\n",
        "  test = data[~data.index.isin(train.index)].copy()\n",
        "\n",
        "  print('# training counties:', len(train_counties),\n",
        "        '\\n# training zip codes:', train.shape[0],\n",
        "        '\\n# test zip codes:', test.shape[0])\n",
        "  return train, test\n",
        "\n",
        "def run_imputation_model(\n",
        "    train,\n",
        "    test,\n",
        "    label,\n",
        "    min_population=500,\n",
        "    model_class=linear_model.Ridge,\n",
        "    model_kwargs={}):\n",
        "  \"\"\"\n",
        "  Runs the imputation model.\n",
        "  \"\"\"\n",
        "  train = train[(train.population >= min_population) & train[label].notna()].copy()\n",
        "  test = test[(test.population >= min_population) & test[label].notna()].copy()\n",
        "\n",
        "  feature_data_train = train['features'].apply(\n",
        "      lambda x: pd.Series(x, index=embedding_features))\n",
        "  feature_data_test = test['features'].apply(\n",
        "      lambda x: pd.Series(x, index=embedding_features))\n",
        "\n",
        "  train = pd.concat([train.drop(columns=['features'], errors='ignore'), feature_data_train], axis=1)\n",
        "  test = pd.concat([test.drop(columns=['features'], errors='ignore'), feature_data_test], axis=1)\n",
        "\n",
        "  model = make_pipeline(preprocessing.MinMaxScaler(),\n",
        "                        model_class(**model_kwargs))\n",
        "  model.fit(train[embedding_features], train[label])\n",
        "  predictions = model.predict(test[embedding_features])\n",
        "\n",
        "  gdf_predictions = make_predictions_df(predictions, test, label)\n",
        "  results = evaluate(gdf_predictions)\n",
        "  return model, results\n",
        "\n",
        "\n",
        "# Increasing this value generally improves performance.\n",
        "training_fraction = 0.2\n",
        "label = 'Percent_Person_WithHigherEdu'\n",
        "train, test = get_train_test_split(training_fraction)\n",
        "model, results = run_imputation_model(train, test, label)\n",
        "results"
      ],
      "metadata": {
        "id": "csYlgZKLqFoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualize a few counties from the test set.\n",
        "test_counties = test.county_id.unique()\n",
        "large_counties = df[df.county_id.isin(test_counties)].sort_values(\n",
        "    'population', ascending=False)[['state', 'county', 'population']].head(4)\n",
        "for _, row in large_counties.iterrows():\n",
        "  _ = subset_eval(label, row.county, row.state, gdf_predictions, cmap='Blues')"
      ],
      "metadata": {
        "id": "tx-tx4uz3Etr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Try other labels.\n",
        "labels = [\n",
        "          'Percent_Person_WithHigherEdu',\n",
        "          'Median_Age_Person',\n",
        "          'Median_Income_Household',\n",
        "          'Percent_Person_WithAsthma',\n",
        "          'Percent_Person_WithHighBloodPressure'\n",
        "]\n",
        "train, test = get_train_test_split(0.8)\n",
        "models_by_label = {}\n",
        "metrics_df = pd.DataFrame(\n",
        "    columns=['label', 'r2', 'rmse', 'mae', 'mape', 'model'])\n",
        "for label in labels:\n",
        "  models_by_label[label], results = run_imputation_model(train, test, label)\n",
        "  results['label'] = label\n",
        "  results['model'] = 'linear'\n",
        "  metrics_df.loc[len(metrics_df)] = results\n",
        "\n",
        "metrics_df.round(3)"
      ],
      "metadata": {
        "id": "3bFwhB0E2qzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Try LightGBM models instead of linear.\n",
        "\n",
        "# This will take a few minutes to run.\n",
        "models_by_label_lgbm = {}\n",
        "metrics_df_lgbm = pd.DataFrame(\n",
        "    columns=['label', 'r2', 'rmse', 'mae', 'mape', 'model'])\n",
        "for label in labels:\n",
        "  models_by_label_lgbm[label], results = run_imputation_model(\n",
        "      train, test, label, model_class=lgbm.LGBMRegressor,\n",
        "      model_kwargs={'min_child_samples': 40,\n",
        "                    'importance_type': 'gain',\n",
        "                    'n_estimators': 400,\n",
        "                    'learning_rate': 0.04,\n",
        "                    'force_col_wise': True,\n",
        "                    })\n",
        "  results['label'] = label\n",
        "  results['model'] = 'lgbm'\n",
        "  metrics_df_lgbm.loc[len(metrics_df_lgbm)] = results\n",
        "\n",
        "metrics_df_lgbm.round(3)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8NlkBAcl0UZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LGBM results are mostly comparable with the linear model. They can be improved with more iterations and lower learning rate. You can also try setting `feature_fraction=0.5`."
      ],
      "metadata": {
        "id": "NlKp3P_oQBFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title LightGBM feature importance\n",
        "import seaborn as sns\n",
        "features = {\n",
        "  'trends': (128, embedding_features[:128]),\n",
        "  'maps': (128, embedding_features[128:256]),\n",
        "  'weather': (74, embedding_features[256:]),\n",
        "}\n",
        "all_importance = []\n",
        "for label, model in models_by_label_lgbm.items():\n",
        "  importance = pd.DataFrame(model[1].feature_importances_,\n",
        "                            index=embedding_features,\n",
        "                            columns=['importance'])\n",
        "  importance['importance'] = importance['importance'].abs()\n",
        "  for feature, dims in features.items():\n",
        "    importance.loc[dims[1], 'feature'] = feature\n",
        "  importance = importance.groupby('feature').importance.sum().reset_index()\n",
        "  importance['importance'] = (importance.importance /\n",
        "                              importance.importance.sum() * 100)\n",
        "  importance['label'] = label\n",
        "  all_importance.append(importance)\n",
        "all_importance = pd.concat(all_importance)\n",
        "_, ax = plt.subplots(figsize=(10, 3))\n",
        "sns.barplot(data=all_importance, x='label', y='importance',\n",
        "            hue='feature',\n",
        "            hue_order=features.keys(), ax=ax)\n",
        "_ = plt.xticks(rotation=30)"
      ],
      "metadata": {
        "id": "EdnJWqnlzv5O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}